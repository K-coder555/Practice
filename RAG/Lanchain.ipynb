{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ff234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 384\n"
     ]
    }
   ],
   "source": [
    "#ollama model과 HuggingFaceHub 모델을 사용하여 한국어로 질문에 답변하는 챗봇 예제\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "from langchain_community.embeddings import OllamaEmbeddings, HuggingFaceEmbeddings\n",
    "load_dotenv()\n",
    "\n",
    "# Ollama\n",
    "ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vec1 = ollama_emb.embed_query(\"올라마 임베딩 예시\")\n",
    "\n",
    "# Hugging Face\n",
    "hf_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vec2 = hf_emb.embed_query(\"허깅페이스 임베딩 예시\")\n",
    "\n",
    "print(len(vec1), len(vec2))\n",
    "\n",
    "\n",
    "\n",
    "# 전역 메모리 객체\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,  # 최근 5개의 대화만 기억\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df08961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_documents():\n",
    "    #문서 로드 및 분할\n",
    "    print(\"문서 로드하는 중...\")\n",
    "    \n",
    "    # 샘플 문서\n",
    "    documents = [\n",
    "        Document(page_content=\"LangChain은 파이썬 기반의 LLM 프레임워크입니다. 다양한 AI 모델과 연동할 수 있습니다.\"),\n",
    "        Document(page_content=\"LCEL은 LangChain Expression Language의 줄임말로, 복잡한 체인을 쉽게 구성할 수 있게 해주는 언어입니다.\"),\n",
    "        Document(page_content=\"RAG는 Retrieval Augmented Generation의 줄임말로, 외부 문서를 검색하여 답변의 정확성을 높이는 기법입니다.\"),\n",
    "        Document(page_content=\"벡터 데이터베이스는 텍스트를 벡터로 변환하여 의미론적 유사성을 기반으로 검색할 수 있게 해줍니다.\"),\n",
    "        Document(page_content=\"프롬프트 엔지니어링은 AI 모델에게 더 나은 응답을 얻기 위해 입력을 최적화하는 기법입니다.\")\n",
    "    ]\n",
    "    \n",
    "    # 텍스트 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"문서를 {len(splits)}개의 청크로 분할했습니다.\")\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "327be8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "def create_vector_store(documents):\n",
    "    #벡터 스토어 생성\n",
    "    print(\"벡터 스토어 생성 중...\")\n",
    "    \n",
    "    query = \"LangChain과 LCEL, RAG가 무엇인가요?\"\n",
    "    # Ollama\n",
    "    ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    # Hugging Face\n",
    "    hf_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "    print(\"---\")\n",
    "    print(\"OLLAMA 벡터 스토어 생성 완료!\")\n",
    "    print(\"---\")\n",
    "    vector_store_ol = FAISS.from_documents(documents=documents, embedding=ollama_emb)\n",
    "\n",
    "\n",
    "    qvec1 = np.array(ollama_emb.embed_query(query)).reshape(1, -1)\n",
    "\n",
    "    # 모든 문서 벡터 꺼내기\n",
    "    doc_vec1 = np.vstack([vector_store_ol.index.reconstruct(i) for i in range(vector_store_ol.index.ntotal)])\n",
    "    sim1 = cosine_similarity(qvec1, doc_vec1)[0]\n",
    "\n",
    "    # 상위 3개만 추출\n",
    "    top_idx1 = sim1.argsort()[::-1][:3]\n",
    "    print(\"Example query:\", query)\n",
    "    print(\"---\")\n",
    "    for i, idx in enumerate(top_idx1, 1):\n",
    "        doc_id = vector_store_ol.index_to_docstore_id[idx]\n",
    "        doc = vector_store_ol.docstore.search(doc_id)\n",
    "        print(f\"#{i} Cosine similarity: {sim1[idx]:.4f}\")\n",
    "        print(f\"#{i} 문서: {doc.page_content[:100]}...\")\n",
    "        print(\"---\")\n",
    "\n",
    "    #print(\"---\")\n",
    "    print(\"Hugging face 벡터 스토어 생성 완료!\")\n",
    "    print(\"---\")\n",
    "    vector_store_hf = FAISS.from_documents(documents=documents, embedding=hf_emb)\n",
    "    qvec2 = np.array(hf_emb.embed_query(str(query))).reshape(1, -1)\n",
    "\n",
    "    # 모든 문서 벡터 꺼내기\n",
    "    doc_vec2 = np.vstack([vector_store_hf.index.reconstruct(i) for i in range(vector_store_hf.index.ntotal)])\n",
    "    sim2 = cosine_similarity(qvec2, doc_vec2)[0]\n",
    "\n",
    "    # 상위 3개만 추출\n",
    "    top_idx2 = sim2.argsort()[::-1][:3]\n",
    "    print(\"Example query:\", query)\n",
    "    print(\"---\")\n",
    "\n",
    "    for i, idx in enumerate(top_idx2, 1):\n",
    "        doc_id = vector_store_hf.index_to_docstore_id[idx]\n",
    "        doc = vector_store_hf.docstore.search(doc_id)\n",
    "        print(f\"#{i} Cosine similarity: {sim2[idx]:.4f}\")\n",
    "        print(f\"#{i} 문서: {doc.page_content[:100]}...\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    return vector_store_ol, vector_store_hf\n",
    "\n",
    "def format_docs(docs):\n",
    "    #검색된 문서들을 문자열로 포맷팅\n",
    "    return \"\\n\\n--- 문서 내용 ---\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9bdd520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_history():\n",
    "    #채팅 히스토리를 문자열로 변환\n",
    "    messages = memory.chat_memory.messages\n",
    "    if not messages:\n",
    "        return \"이전 대화 기록이 없습니다.\"\n",
    "    \n",
    "    history_str = []\n",
    "    for msg in messages[-6:]:  # 최근 3턴만\n",
    "        if hasattr(msg, 'type'):\n",
    "            if msg.type == 'human':\n",
    "                history_str.append(f\"사용자: {msg.content}\")\n",
    "            elif msg.type == 'ai':\n",
    "                history_str.append(f\"AI: {msg.content}\")\n",
    "    \n",
    "    return \"\\n\".join(history_str) if history_str else \"이전 대화 기록이 없습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bdfdcdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain(vector_store):\n",
    "    #LCEL을 사용한 RAG 체인 생성\n",
    "    \n",
    "    # LLM 초기화\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # 검색기 설정\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    # 프롬프트 템플릿\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "당신은 문서의 내용을 바탕으로 질문에 답변하는 도우미입니다.\n",
    "주어진 문맥 정보와 이전 대화 기록을 모두 고려하여 정확하고 도움이 되는 답변을 한글로 제공해주세요.\n",
    "\n",
    "이전 대화 기록:\n",
    "{chat_history}\n",
    "\n",
    "관련 문서 내용:\n",
    "{context}\n",
    "\n",
    "현재 질문: {question}\n",
    "\n",
    "답변 시 다음 사항을 고려해주세요:\n",
    "- 문서의 내용을 정확하게 반영하여 답변하세요\n",
    "- 이전 대화의 맥락을 고려하여 답변하세요\n",
    "- 구체적인 예시나 데이터가 있다면 포함해주세요\n",
    "- 답변은 한국어로 명확하고 이해하기 쉽게 작성해주세요\n",
    "- 문서에 없는 내용은 추측하지 마세요\n",
    "- 답변은 500자 이내로 간결하게 작성해주세요\n",
    "\n",
    "답변:\n",
    "\"\"\")\n",
    "    \n",
    "    # LCEL 체인 구성\n",
    "    chain = (\n",
    "        RunnableParallel({\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"chat_history\": lambda x: get_chat_history()\n",
    "        })\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eb2c4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(chain, question):\n",
    "    #질문하고 답변받기 (메모리 업데이트 포함)\n",
    "    print(f\"\\n질문: {question}\")\n",
    "\n",
    "    # 체인 실행\n",
    "    response = chain.invoke(question)\n",
    "        \n",
    "    # 메모리에 대화 기록 저장\n",
    "    memory.chat_memory.add_user_message(question)\n",
    "    memory.chat_memory.add_ai_message(response)\n",
    "        \n",
    "    print(f\"답변: {response}\")\n",
    "    print(f\"[메모리] 저장된 대화 수: {len(memory.chat_memory.messages)//2}턴\")\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e41f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_memory():\n",
    "    #메모리 내용 출력\n",
    "    messages = memory.chat_memory.messages\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"저장된 대화가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    for i, msg in enumerate(messages):\n",
    "        if hasattr(msg, 'type'):\n",
    "            if msg.type == 'human':\n",
    "                print(f\"#{i//2 + 1} 질문: {msg.content}\")\n",
    "            elif msg.type == 'ai':\n",
    "                print(f\"#{i//2 + 1} 답변: {msg.content[:100]}...\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3ad9373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 로드하는 중...\n",
      "문서를 5개의 청크로 분할했습니다.\n",
      "벡터 스토어 생성 중...\n",
      "---\n",
      "OLLAMA 벡터 스토어 생성 완료!\n",
      "---\n",
      "Example query: LangChain과 LCEL, RAG가 무엇인가요?\n",
      "---\n",
      "#1 Cosine similarity: 0.7687\n",
      "#1 문서: LCEL은 LangChain Expression Language의 줄임말로, 복잡한 체인을 쉽게 구성할 수 있게 해주는 언어입니다....\n",
      "---\n",
      "#2 Cosine similarity: 0.7259\n",
      "#2 문서: LangChain은 파이썬 기반의 LLM 프레임워크입니다. 다양한 AI 모델과 연동할 수 있습니다....\n",
      "---\n",
      "#3 Cosine similarity: 0.6068\n",
      "#3 문서: RAG는 Retrieval Augmented Generation의 줄임말로, 외부 문서를 검색하여 답변의 정확성을 높이는 기법입니다....\n",
      "---\n",
      "Hugging face 벡터 스토어 생성 완료!\n",
      "---\n",
      "Example query: LangChain과 LCEL, RAG가 무엇인가요?\n",
      "---\n",
      "#1 Cosine similarity: 0.7397\n",
      "#1 문서: LCEL은 LangChain Expression Language의 줄임말로, 복잡한 체인을 쉽게 구성할 수 있게 해주는 언어입니다....\n",
      "---\n",
      "#2 Cosine similarity: 0.6595\n",
      "#2 문서: LangChain은 파이썬 기반의 LLM 프레임워크입니다. 다양한 AI 모델과 연동할 수 있습니다....\n",
      "---\n",
      "#3 Cosine similarity: 0.4327\n",
      "#3 문서: 프롬프트 엔지니어링은 AI 모델에게 더 나은 응답을 얻기 위해 입력을 최적화하는 기법입니다....\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 문서 로드 및 처리\n",
    "    documents = load_and_process_documents()\n",
    "    \n",
    "    # 2. 벡터 스토어 생성\n",
    "    vector_store_ol, vector_store_hf = create_vector_store(documents)\n",
    "    \n",
    "    # 3. RAG 체인 생성\n",
    "    rag_chain_ol = create_rag_chain(vector_store_ol)\n",
    "    rag_chain_hf = create_rag_chain(vector_store_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4f62f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OLLAMA Text Embedding | LCEL 기반 Multi-Turn RAG 시스템 준비완료.\n",
      "============================================================\n",
      "\n",
      "==================== 턴 1 ====================\n",
      "\n",
      "질문: LangChain과 LCEL, RAG가 무엇인가요?\n",
      "답변: LangChain은 파이썬 기반의 LLM(대형 언어 모델) 프레임워크로, 다양한 AI 모델과 연동하여 사용할 수 있는 플랫폼입니다. LCEL은 LangChain Expression Language의 줄임말로, 복잡한 체인을 쉽게 구성할 수 있도록 도와주는 언어입니다. RAG는 Retrieval Augmented Generation의 약자로, 외부 문서를 검색하여 답변의 정확성을 높이는 기법입니다. 이 세 가지 요소는 함께 작동하여 AI 모델의 성능을 향상시키고, 사용자에게 더 정확하고 유용한 정보를 제공하는 데 기여합니다.\n",
      "[메모리] 저장된 대화 수: 1턴\n",
      "\n",
      "==================== 턴 2 ====================\n",
      "\n",
      "질문: 지금까지 설명한 기술들을 어떻게 함께 사용할 수 있나요?\n",
      "답변: 지금까지 설명한 기술들은 함께 사용하여 AI 모델의 성능을 극대화할 수 있습니다. 예를 들어, LangChain을 활용하여 다양한 AI 모델을 통합하고, LCEL을 사용해 복잡한 체인을 쉽게 구성할 수 있습니다. 이 과정에서 프롬프트 엔지니어링 기법을 적용하여 입력을 최적화하면 AI 모델이 더 나은 응답을 생성할 수 있습니다. 또한, RAG 기법을 통해 외부 문서를 검색하여 답변의 정확성을 높일 수 있습니다. 마지막으로, 벡터 데이터베이스를 활용하여 텍스트를 벡터로 변환하고 의미론적 유사성을 기반으로 검색함으로써, 더욱 정교하고 관련성 높은 정보를 제공할 수 있습니다. 이러한 기술들이 결합되면, 사용자에게 더 유용하고 정확한 결과를 제공할 수 있습니다.\n",
      "[메모리] 저장된 대화 수: 2턴\n",
      "\n",
      "==================== 최종 메모리 상태 ====================\n",
      "#1 질문: LangChain과 LCEL, RAG가 무엇인가요?\n",
      "#1 답변: LangChain은 파이썬 기반의 LLM(대형 언어 모델) 프레임워크로, 다양한 AI 모델과 연동하여 사용할 수 있는 플랫폼입니다. LCEL은 LangChain Expression...\n",
      "#2 질문: 지금까지 설명한 기술들을 어떻게 함께 사용할 수 있나요?\n",
      "#2 답변: 지금까지 설명한 기술들은 함께 사용하여 AI 모델의 성능을 극대화할 수 있습니다. 예를 들어, LangChain을 활용하여 다양한 AI 모델을 통합하고, LCEL을 사용해 복잡한 ...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OLLAMA Text Embedding | LCEL 기반 Multi-Turn RAG 시스템 준비완료.\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "# 4. Multi-Turn 대화 예시\n",
    "conversation_flow = [\n",
    "    \"LangChain과 LCEL, RAG가 무엇인가요?\",\n",
    "    \"지금까지 설명한 기술들을 어떻게 함께 사용할 수 있나요?\"\n",
    "]\n",
    "    \n",
    "for i, question in enumerate(conversation_flow, 1):\n",
    "    print(f\"\\n{'='*20} 턴 {i} {'='*20}\")\n",
    "    ask_question(rag_chain_ol, question)\n",
    "        \n",
    "    if i == 3:  # 3턴 후 메모리 상태 확인\n",
    "        show_memory()\n",
    "    \n",
    "# 5. 최종 메모리 상태\n",
    "print(f\"\\n{'='*20} 최종 메모리 상태 {'='*20}\")\n",
    "show_memory()\n",
    "memory.clear()  # 메모리 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3028fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Hugging Face Text Embedding | LCEL 기반 Multi-Turn RAG 시스템 준비완료.\n",
      "============================================================\n",
      "\n",
      "==================== 턴 1 ====================\n",
      "\n",
      "질문: LangChain과 LCEL, RAG가 무엇인가요?\n",
      "답변: LangChain은 파이썬 기반의 LLM(대형 언어 모델) 프레임워크로, 다양한 AI 모델과 연동하여 복잡한 작업을 수행할 수 있도록 돕습니다. LCEL은 LangChain Expression Language의 약자로, 복잡한 체인을 쉽게 구성할 수 있게 해주는 언어입니다. 이를 통해 사용자는 더 효율적으로 AI 모델을 활용할 수 있습니다. \n",
      "\n",
      "RAG(복합적 응답 생성)는 문서에 명시되어 있지 않지만, 일반적으로 정보 검색과 생성 모델을 결합하여 더 정확하고 유용한 응답을 생성하는 기법을 의미합니다. 이러한 기법은 프롬프트 엔지니어링과 함께 사용되어 AI 모델의 응답 품질을 향상시키는 데 기여합니다.\n",
      "[메모리] 저장된 대화 수: 1턴\n",
      "\n",
      "==================== 턴 2 ====================\n",
      "\n",
      "질문: 지금까지 설명한 기술들을 어떻게 함께 사용할 수 있나요?\n",
      "답변: LangChain, LCEL, RAG, 벡터 데이터베이스, 그리고 프롬프트 엔지니어링은 함께 사용하여 AI 모델의 성능을 극대화할 수 있습니다. \n",
      "\n",
      "예를 들어, LangChain을 사용하여 다양한 AI 모델과 연동하고, LCEL을 통해 복잡한 체인을 구성할 수 있습니다. 이 과정에서 벡터 데이터베이스를 활용하여 텍스트를 벡터로 변환하고, 의미론적 유사성을 기반으로 관련 정보를 검색합니다. \n",
      "\n",
      "그 후, 프롬프트 엔지니어링 기법을 적용하여 입력을 최적화함으로써 AI 모델이 더 나은 응답을 생성하도록 유도할 수 있습니다. RAG 기법을 통해 정보 검색과 생성 모델을 결합하면, 더욱 정확하고 유용한 응답을 얻을 수 있습니다. 이러한 통합적인 접근 방식은 AI의 활용도를 높이는 데 기여합니다.\n",
      "[메모리] 저장된 대화 수: 2턴\n",
      "\n",
      "==================== 최종 메모리 상태 ====================\n",
      "#1 질문: LangChain과 LCEL, RAG가 무엇인가요?\n",
      "#1 답변: LangChain은 파이썬 기반의 LLM(대형 언어 모델) 프레임워크로, 다양한 AI 모델과 연동하여 복잡한 작업을 수행할 수 있도록 돕습니다. LCEL은 LangChain Exp...\n",
      "#2 질문: 지금까지 설명한 기술들을 어떻게 함께 사용할 수 있나요?\n",
      "#2 답변: LangChain, LCEL, RAG, 벡터 데이터베이스, 그리고 프롬프트 엔지니어링은 함께 사용하여 AI 모델의 성능을 극대화할 수 있습니다. \n",
      "\n",
      "예를 들어, LangChain을...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Hugging Face Text Embedding | LCEL 기반 Multi-Turn RAG 시스템 준비완료.\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "# 4. Multi-Turn 대화 예시\n",
    "conversation_flow = [\n",
    "    \"LangChain과 LCEL, RAG가 무엇인가요?\",\n",
    "    \"지금까지 설명한 기술들을 어떻게 함께 사용할 수 있나요?\"\n",
    "]\n",
    "    \n",
    "for i, question in enumerate(conversation_flow, 1):\n",
    "    print(f\"\\n{'='*20} 턴 {i} {'='*20}\")\n",
    "    ask_question(rag_chain_hf, question)\n",
    "        \n",
    "    if i == 3:  # 3턴 후 메모리 상태 확인\n",
    "        show_memory()\n",
    "    \n",
    "# 5. 최종 메모리 상태\n",
    "print(f\"\\n{'='*20} 최종 메모리 상태 {'='*20}\")\n",
    "show_memory()\n",
    "memory.clear()  # 메모리 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeec37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
